% Proposed Algorithm for Inter-Turn Short-Circuit Fault Detection in the
%     DFIG's Stator using EMD-Based Statistical Features and LSTM,
%     evaluated using K-fold cross-validation
% The number of features selected (using either filter or proposed hybrid methods)
%     can be set.
% This routine can be used to reproduce the results reported in the first column of tables 4 and 6
%     of the following paper:
%     Ghasem Alipoor, Seyed Jafar Mirbagheri, Seyed Mohammad Mahdi Moosavi and Sergio M. A. Cruz,
%     “Incipient Detection of Stator Inter-Turn Short-Circuit Faults in a DFIG Using Deep Learning,”
%     accepted for publication in the IET Electric Power Applications, DOI: 10.1049/elp2.12262.
% 
% Generated by G. Alipoor (alipoor@hut.ac.ir)
% Last Modifications October, 23rd, 2022
%

clearvars
close all
clc

%% Settings
TestVersion = '';

rng('default');

InputDirectory = 'Dataset';

Frame_Length = 360;
NumIMFs = 5;
NumFolds = 10;
NofSelectedFeatures = [150, 6, 20, 21, 30];
Comment = '';   % An arbitrary comment, to saved with results

% Set the model's hyper-parameters:
Model_Hyperparams.numClasses         = 6;
Model_Hyperparams.numLSTMs          = 1;
Model_Hyperparams.numHiddenUnits = 150;

% Set the learning parameters:
Learning_Params.miniBatchSize = 144;
Learning_Params.numEpochs = 300;
Learning_Params.learnRate = 0.001;
Learning_Params.gradientThreshold = 2;
Learning_Params.gradientDecayFactor = 0.9;
Learning_Params.squaredGradientDecayFactor = 0.999;

ResultsFile = sprintf('Results\\Proposed_FrmLen%d_KFold_%s', ...
    Frame_Length, TestVersion);
% Settings, to be saved with results
Settings = struct('Model_Hyperparams', Model_Hyperparams, ...
    'Learning_Params', Learning_Params, ...
    'Frame_Length', Frame_Length, ...
    'NumIMFs', NumIMFs, ...
    'SelectedSignalsIndices', SelectedSignalsIndices, ...
    'NofSelectedFeatures', NofSelectedFeatures);

if isscalar(Model_Hyperparams.numHiddenUnits)
    Model_Hyperparams.numHiddenUnits = ...
        repmat(Model_Hyperparams.numHiddenUnits, Model_Hyperparams.numLSTMs, 1);
elseif length(Model_Hyperparams.numHiddenUnits) ~= Model_Hyperparams.numLSTMs
    error('Hidden unit sizes of the lstm layers are not specified properly.)')
end

%% Load Data
FeaturesFile = sprintf('Features\\Features_FrmLen%d.mat', Frame_Length);
if ~exist(FeaturesFile, 'file')
    FeatureExtraction(InputDirectory, Frame_Length, NumIMFs)
end
Features = load(FeaturesFile, 'Data', 'Label', 'Comment');
Data  = Features.Data;
Label = Features.Label;
Data = reshape(Data, size(Data, 1), 10, size(Data, 2)/10);
Label = Label(1:10:end);
Comment = sprintf('%s\n %s', Comment, Features.Comment);
clear Features
% Data = Data(:, :, 1:50:end); Label = Label(1:50:end);

% Load the features sorted based on filter and hybrid feature selection methods
Temp = load(sprintf('Results\\FeatureSelection_Hybrid_FrmLen%d', ...
    Frame_Length), 'Results');
SortedFeatures.Filter = Temp.Results.SortedFeatures;
SortedFeatures.Hybrid = Temp.Results.SelectedFeatures;
clear Temp

[InputSize, SeqLength, N_Samples] = size(Data);
N_Samples = NumFolds*floor(N_Samples/NumFolds);
Data = Data(:, :, 1:N_Samples);
CVO = cvpartition(N_Samples, 'KFold', NumFolds);  % Cross-Validation Object

for i_NFeatures = 1:length(NofSelectedFeatures)
    %% Using features selected by the filter method
    % Discarding non-selected features
    InputSize = NofSelectedFeatures(i_NFeatures);
    if NofSelectedFeatures(i_NFeatures) == 150
        fprintf('Using All Features\n')
        SelectedFeatures = 1:150;
        Accuracy.All = zeros(1, NumFolds);
    else
        fprintf('Using %d Features, Filter Method:\n', NofSelectedFeatures(i_NFeatures))
        SelectedFeatures = SortedFeatures.Filter(1:NofSelectedFeatures(i_NFeatures));
        eval(['Accuracy.Filter_' num2str(NofSelectedFeatures(i_NFeatures)) 'Features = zeros(1, NumFolds);']);
    end
    
    for i_KFold = 1:NumFolds
        fprintf('\tFold %2d:', i_KFold)
        
        XTrain = squeeze(num2cell(Data(SelectedFeatures, :, CVO.training(i_KFold)), [1 2]));
        YTrain = categorical(Label(CVO.training(i_KFold)));
        
        XTest = squeeze(num2cell(Data(SelectedFeatures, :, CVO.test(i_KFold)), [1 2]));
        YTest = categorical(Label(CVO.test(i_KFold)));
        
        %% Train
        % Defining the model
        layers = sequenceInputLayer(InputSize);
        for i_LSTM = 1:Model_Hyperparams.numLSTMs -1
            layers = cat(1, layers, lstmLayer(Model_Hyperparams.numHiddenUnits(i_LSTM), 'OutputMode', 'sequence'));
        end
        layers = cat(1, layers, [lstmLayer(Model_Hyperparams.numHiddenUnits(end), 'OutputMode', 'last')
            fullyConnectedLayer(Model_Hyperparams.numClasses)
            softmaxLayer
            classificationLayer]);
        
        % Training
        options = trainingOptions('adam', ...
            'MaxEpochs', Learning_Params.numEpochs, ...
            'MiniBatchSize', Learning_Params.miniBatchSize, ...
            'Shuffle', 'every-epoch', ...
            'InitialLearnRate', Learning_Params.learnRate, ...
            'GradientDecayFactor', Learning_Params.gradientDecayFactor, ...
            'SquaredGradientDecayFactor', Learning_Params.squaredGradientDecayFactor, ...
            'GradientThreshold', Learning_Params.gradientThreshold, ...
            'Verbose', 0, ...
            'Plots', 'none');
        
        net = trainNetwork(XTrain, YTrain, layers, options);
        
        %% Test
        YPred = classify(net, XTest, 'MiniBatchSize', Learning_Params.miniBatchSize);
        if NofSelectedFeatures(i_NFeatures) == 150
            Accuracy.All(i_KFold) = 100*sum(YPred == YTest)./numel(YTest);
        else
            eval(['Accuracy.Filter_' num2str(NofSelectedFeatures(i_NFeatures)) 'Features(i_KFold) = 100*sum(YPred == YTest)./numel(YTest);']);
        end
        
        fprintf('\t Accuracy = %4.2f\n', 100*sum(YPred == YTest)./numel(YTest))
    end
    %% Displying the averaged results
    if NofSelectedFeatures(i_NFeatures) == 150
        fprintf('Averaged Accuracy = %4.2f\n', mean(Accuracy.All))
    else
        fprintf('Averaged Accuracy = %4.2f\n', eval(['mean(Accuracy.Filter_' num2str(NofSelectedFeatures(i_NFeatures)) 'Features)']))
    end
    
    %% Using features selected by the hybrid method
    % Discarding non-selected features
    InputSize = NofSelectedFeatures(i_NFeatures);
    if NofSelectedFeatures(i_NFeatures) ~= 150
        fprintf('Using %d Features, Hybrid Method:\n', NofSelectedFeatures(i_NFeatures))
        SelectedFeatures = SortedFeatures.Hybrid(1:NofSelectedFeatures(i_NFeatures));
        eval(['Accuracy.Hybrid_' num2str(NofSelectedFeatures(i_NFeatures)) 'Features = zeros(1, NumFolds);']);
        
        for i_KFold = 1:NumFolds
            fprintf('\tFold %d:', i_KFold)
            
            XTrain = squeeze(num2cell(Data(SelectedFeatures, :, CVO.training(i_KFold)), [1 2]));
            YTrain = categorical(Label(CVO.training(i_KFold)));
            
            XTest = squeeze(num2cell(Data(SelectedFeatures, :, CVO.test(i_KFold)), [1 2]));
            YTest = categorical(Label(CVO.test(i_KFold)));
            
            %% Train
            % Defining the model
            layers = sequenceInputLayer(InputSize);
            for i_LSTM = 1:Model_Hyperparams.numLSTMs -1
                layers = cat(1, layers, lstmLayer(Model_Hyperparams.numHiddenUnits(i_LSTM), 'OutputMode', 'sequence'));
            end
            layers = cat(1, layers, [lstmLayer(Model_Hyperparams.numHiddenUnits(end), 'OutputMode', 'last')
                fullyConnectedLayer(Model_Hyperparams.numClasses)
                softmaxLayer
                classificationLayer]);
            
            % Training
            options = trainingOptions('adam', ...
                'MaxEpochs', Learning_Params.numEpochs, ...
                'MiniBatchSize', Learning_Params.miniBatchSize, ...
                'Shuffle', 'every-epoch', ...
                'InitialLearnRate', Learning_Params.learnRate, ...
                'GradientDecayFactor', Learning_Params.gradientDecayFactor, ...
                'SquaredGradientDecayFactor', Learning_Params.squaredGradientDecayFactor, ...
                'GradientThreshold', Learning_Params.gradientThreshold, ...
                'Verbose', 0, ...
                'Plots', 'none');
            
            net = trainNetwork(XTrain, YTrain, layers, options);
            
            %% Test
            YPred = classify(net, XTest, 'MiniBatchSize', Learning_Params.miniBatchSize);
            if NofSelectedFeatures(i_NFeatures) == 150
                Accuracy.All(i_KFold) = 100*sum(YPred == YTest)./numel(YTest);
            else
                eval(['Accuracy.Hybrid_' num2str(NofSelectedFeatures(i_NFeatures)) 'Features(i_KFold) = 100*sum(YPred == YTest)./numel(YTest);']);
            end
            
            fprintf('\t Accuracy = %4.2f\n', 100*sum(YPred == YTest)./numel(YTest))
        end
        %% Displying the averaged results
        if NofSelectedFeatures(i_NFeatures) == 150
            fprintf('Averaged Accuracy = %4.2f\n', mean(Accuracy.All))
        else
            fprintf('Averaged Accuracy = %4.2f\n', eval(['mean(Accuracy.Hybrid_' num2str(NofSelectedFeatures(i_NFeatures)) 'Features)']))
        end
    end
    
    save([ResultsFile '.mat'], 'Accuracy', 'Settings', 'Comment');
end
