% Test of the Presented Hybrid Feature Selection Method Used in the
%     Proposed Algorithm for Inter-Turn Short-Circuit Fault Detection in the
%     DFIG's Stator using EMD-Based Statistical Features and LSTM
% This routine can be used to reproduce the results reported in figure 8.b and table 5 of the following paper:
%     Ghasem Alipoor, Seyed Jafar Mirbagheri, Seyed Mohammad Mahdi Moosavi and Sergio M. A. Cruz,
%     “Incipient Detection of Stator Inter-Turn Short-Circuit Faults in a DFIG Using Deep Learning,”
%     accepted for publication in the IET Electric Power Applications, DOI: 10.1049/elp2.12262.
% 
% Generated by G. Alipoor (alipoor@hut.ac.ir)
% Last Modifications October, 23rd, 2022
%

clearvars
close all
clc

%% Settings
TestVersion = '';

rng('default');     % For repeatability

InputDirectory = 'Dataset';

%'StatorVoltages', 'StatorCurrents', 'RotorCurrents', 'IDQR', 'AllCases' or any other set of signals
Frame_Length = 360;
NumIMFs = 5;
NumFolds = 4;
SelectionMethod = 'NCA';    % Filter-based feature selection method, either:
%                 MRMR (for Minimum Redundancy Maximum Relevance)
%                 NCA (for Neighborhood Component Analysis)
NumCandidatFeatures = 5;       % Number of features checked in each step of feature selection
RegPar = .1;       % Regularization parameter used in hybrid feature selection
Comment = '';   % An arbitrary comment, to saved with results

% Set the model's hyper-parameters:
Model_Hyperparams.numClasses = 6;
Model_Hyperparams.numLSTMs  = 1;

% Set the learning parameters:
Model_Hyperparams.numHiddenUnits = 150;
Model_Hyperparams.miniBatchSize = 144;
Learning_Params.DroupoutProb = 0.2;
Learning_Params.L2Regularization = .0001;
Learning_Params.numEpochs = 50;
Learning_Params.learnRate = 0.001;
Learning_Params.gradientThreshold = 2;
Learning_Params.gradientDecayFactor = 0.9;
Learning_Params.squaredGradientDecayFactor = 0.999;


ResultsFile = sprintf('Results\\FeatureSelection_Hybrid_%s_FrmLen%d_%s', ...
    SelectionMethod, Frame_Length, TestVersion);
% Settings, to be saved with results
Settings = struct('SelectionMethod', SelectionMethod, ...
    'Model_Hyperparams', Model_Hyperparams, ...
    'Learning_Params', Learning_Params, ...
    'Frame_Length', Frame_Length, ...
    'NumIMFs', NumIMFs);

if isscalar(Model_Hyperparams.numHiddenUnits)
    Model_Hyperparams.numHiddenUnits = ...
        repmat(Model_Hyperparams.numHiddenUnits, Model_Hyperparams.numLSTMs, 1);
elseif length(Model_Hyperparams.numHiddenUnits) ~= Model_Hyperparams.numLSTMs
    error('Hidden unit sizes of the lstm layers are not specified properly.)')
end

%% Load Data
FeaturesFile = sprintf('Features\\Features_FrmLen%d_.mat', Frame_Length);
if ~exist(FeaturesFile, 'file')
    FeatureExtraction(InputDirectory, Frame_Length, NumIMFs)
end
Features = load(FeaturesFile, 'Data', 'Label', 'Comment');
Data  = Features.Data;
Label = Features.Label;
Data = reshape(Data, size(Data, 1), 10, size(Data, 2)/10);
Label = Label(1:10:end);
% Data = Data(:, :, 1:20:end); Label = Label(1:20:end);
Comment = sprintf('%s\n %s', Comment, Features.Comment);
clear Features

%% Defining the model
% Set the network dimensions
[InputSize, SeqLength, N_Samples] = size(Data);

% Training options
options = trainingOptions('adam', ...
    'MaxEpochs', Learning_Params.numEpochs, ...
    'MiniBatchSize', Model_Hyperparams.miniBatchSize, ...
    'Shuffle', 'every-epoch', ...
    'InitialLearnRate', Learning_Params.learnRate, ...
    'L2Regularization', Learning_Params.L2Regularization, ...
    'GradientDecayFactor', Learning_Params.gradientDecayFactor, ...
    'SquaredGradientDecayFactor', Learning_Params.squaredGradientDecayFactor, ...
    'GradientThreshold', Learning_Params.gradientThreshold, ...
    'ExecutionEnvironment', 'cpu', ...
    'Verbose', 0, ...
    'Plots', 'none');

N_TrainValid = NumFolds*floor(.8*N_Samples/NumFolds);

if ~exist([ResultsFile '.mat'], 'file')
    %% Data parsing for train, validation and test
    shuffledIdx = randperm(N_Samples);
    
    YTest = categorical(Label(shuffledIdx(N_TrainValid + 1:end)));
    
    CVO = cvpartition(N_TrainValid, 'KFold', NumFolds);
    
    %% Sorting Features for Selection
    Scores = zeros(NumFolds, InputSize);
    for i_KFold = 1:NumFolds
        XTemp = reshape(Data(:, :, shuffledIdx(CVO.test(i_KFold))), InputSize, SeqLength*N_TrainValid/NumFolds)';
        YTemp = repmat(Label(shuffledIdx(CVO.test(i_KFold))), 1, SeqLength)';
        YTemp = YTemp(:);
        % Apply feature selection algorithm
        switch SelectionMethod
            case 'MRMR'
                [~,  Scores(i_KFold, :)] = fscmrmr(XTemp, YTemp);
            case 'NCA'
                mdl = fscnca(XTemp, YTemp, 'Standardize',true);
                Scores(i_KFold, :) = mdl.FeatureWeights;
        end
    end
    clear YTemp
    [~, SortedFeatures] = sort(mean(Scores), 2, 'descend');
    Results.SortedFeatures = SortedFeatures;
    
    Results.Accuracy.Valid = zeros(InputSize, NumFolds);
    Results.Accuracy.Test = zeros(InputSize, NumFolds);
    
    SelectedFeatures = [];
    i_Feature = 0;
else
    %% Resuming, in the case of excusion entruption
    load([ResultsFile '.mat'])
    SelectedFeatures = Results.SelectedFeatures;
    SortedFeatures = Results.SortedFeatures;
    [~,Temp] = ismember(SelectedFeatures, SortedFeatures);
    SortedFeatures(Temp) = [];
    YTest = categorical(Label(shuffledIdx(N_TrainValid + 1:end)));
    i_Feature = length(SelectedFeatures);
end

while ~isempty(SortedFeatures)
    i_Feature = i_Feature + 1;
    
    TempAccuracy = zeros(NumCandidatFeatures, NumFolds);
    net = cell(NumCandidatFeatures, NumFolds);
    for i_KFold = 1:NumFolds
        YTrain = categorical(Label(shuffledIdx(CVO.training(i_KFold))));
        YValid = categorical(Label(shuffledIdx(CVO.test(i_KFold))));
        
        for i = 1:NumCandidatFeatures
            %% Training
            XTemp = squeeze(num2cell(Data([SelectedFeatures; SortedFeatures(i)], :, shuffledIdx(CVO.training(i_KFold))), [1 2]));
            
            % Model architecture
            layers = sequenceInputLayer(i_Feature);
            for i_LSTM = 1:Model_Hyperparams.numLSTMs -1
                layers = cat(1, layers, ...
                    lstmLayer(Model_Hyperparams.numHiddenUnits(i_LSTM), 'OutputMode', 'sequence'), ...
                    dropoutLayer(Learning_Params.DroupoutProb));
            end
            layers = cat(1, layers, [lstmLayer(Model_Hyperparams.numHiddenUnits(end), 'OutputMode', 'last')
                fullyConnectedLayer(Model_Hyperparams.numClasses)
                softmaxLayer
                classificationLayer]);
            
            % Train
            net{i, i_KFold} = trainNetwork(XTemp, YTrain, layers, options);
            
            %% Evaluation
            % Accuracy over validation samples
            XTemp = squeeze(num2cell(Data([SelectedFeatures; SortedFeatures(i)], :, shuffledIdx(CVO.test(i_KFold))), [1 2]));
            YPred = classify(net{i}, XTemp, 'MiniBatchSize', Model_Hyperparams.miniBatchSize, 'ExecutionEnvironment', 'cpu');
            TempAccuracy(i, i_KFold) = 100*sum(YPred == YValid)./numel(YValid);
        end
    end
    [~, Temp1] = max(mean(TempAccuracy, 2));
    [~, Temp] = max(mean(TempAccuracy, 2) + ...
        RegPar*100*(NumCandidatFeatures:-1:1)'/NumCandidatFeatures);
    SelectedFeatures = cat(1, SelectedFeatures, SortedFeatures(Temp));
    Results.SelectedFeatures = SelectedFeatures;
    SortedFeatures(Temp) = [];
    Results.Accuracy.Valid(i_Feature, :) = TempAccuracy(Temp, :)';
    
    for i_KFold = 1:NumFolds
        % Accuracy over test samples
        XTemp = squeeze(num2cell(Data(SelectedFeatures, :, shuffledIdx(N_TrainValid + 1:end)), [1 2]));
        YPred = classify(net{Temp, i_KFold}, XTemp, 'MiniBatchSize', Model_Hyperparams.miniBatchSize, 'ExecutionEnvironment', 'cpu');
        Results.Accuracy.Test(i_Feature, i_KFold) = 100*sum(YPred == YTest)./numel(YTest);
    end
    
    % Displaying the result
    fprintf('Accuracy = [%4.2f  %4.2f]\t Selected Feature #%d = %d\t%d   %d\n', ...
        mean(Results.Accuracy.Valid(i_Feature, :), 2), ...
        mean(Results.Accuracy.Test(i_Feature, :), 2), i_Feature, SelectedFeatures(end), Temp, Temp1)
    % Saving the result
    save([ResultsFile '.mat'], 'Results', 'Settings', 'Comment', 'CVO', 'shuffledIdx');
end
% Ploting average results
figure('color', 'w'); grid on; hold on
plot(1:InputSize, mean(Results.Accuracy.Valid, 2), 'LineWidth', 4);
plot(1:InputSize, mean(Results.Accuracy.Test, 2), 'LineWidth', 4);
xlim([1 InputSize])
ylabel('Accuracy', 'FontSize', 18)
xlabel('Number of Selected Features', 'FontSize', 18)
legend('Validation Data', 'Test Data', 'FontSize', 16)
