% Test of the Filter-Based Feature Selection Method Used in the
%     Proposed Algorithm for Inter-Turn Short-Circuit Fault Detection in the
%     DFIG's Stator using EMD-Based Statistical Features and LSTM
% This routine can be used to reproduce the results reported in figure 8.a of the following paper:
%     Ghasem Alipoor, Seyed Jafar Mirbagheri, Seyed Mohammad Mahdi Moosavi and Sergio M. A. Cruz,
%     “Incipient Detection of Stator Inter-Turn Short-Circuit Faults in a DFIG Using Deep Learning,”
%     accepted for publication in the IET Electric Power Applications, DOI: 10.1049/elp2.12262.
% 
% Generated by G. Alipoor (alipoor@hut.ac.ir)
% Last Modifications October, 23rd, 2022
%
clearvars
close all
clc

%% Settings
TestVersion = '';

rng('default');     % For repeatability

InputDirectory = 'Dateset';

Frame_Length = 360;
NumIMFs = 5;
NumFolds = 4;
SelectionMethod = 'NCA';    % Filter-based feature selection method, either:
%                 MRMR (for Minimum Redundancy Maximum Relevance)
%                 NCA (for Neighborhood Component Analysis)
Comment = '';   % An arbitrary comment, to saved with results

% Set the model's hyper-parameters:
Model_Hyperparams.numClasses = 6;
Model_Hyperparams.numLSTMs  = 1;

% Set the learning parameters:
Model_Hyperparams.numHiddenUnits = 150;
Model_Hyperparams.miniBatchSize = 144;
Learning_Params.DroupoutProb = 0.2;
Learning_Params.L2Regularization = .0001;
Learning_Params.numEpochs = 50;
Learning_Params.learnRate = 0.001;
Learning_Params.gradientThreshold = 2;
Learning_Params.gradientDecayFactor = 0.9;
Learning_Params.squaredGradientDecayFactor = 0.999;

ResultsFile = sprintf('Results\\FeatureSelection_Filter_%s_FrmLen%d_%s', ...
    SelectionMethod, Frame_Length, TestVersion);
% Settings, to be saved with results
Settings = struct('SelectionMethod', SelectionMethod, ...
    'Model_Hyperparams', Model_Hyperparams, ...
    'Learning_Params', Learning_Params, ...
    'Frame_Length', Frame_Length, ...
    'NumIMFs', NumIMFs);

if isscalar(Model_Hyperparams.numHiddenUnits)
    Model_Hyperparams.numHiddenUnits = ...
        repmat(Model_Hyperparams.numHiddenUnits, Model_Hyperparams.numLSTMs, 1);
elseif length(Model_Hyperparams.numHiddenUnits) ~= Model_Hyperparams.numLSTMs
    error('Hidden unit sizes of the lstm layers are not specified properly.)')
end

%% Load Data
FeaturesFile = sprintf('Features\\Features_FrmLen%d.mat', Frame_Length);
if ~exist(FeaturesFile, 'file')
    FeatureExtraction(InputDirectory, Frame_Length, NumIMFs)
end
Features = load(FeaturesFile, 'Data', 'Label', 'Comment');
Data  = Features.Data;
Label = Features.Label;
Data = reshape(Data, size(Data, 1), 10, size(Data, 2)/10);
Label = Label(1:10:end);
% Data = Data(:, :, 1:20:end); Label = Label(1:20:end);
Comment = sprintf('%s\n %s', Comment, Features.Comment);
clear Features

%% Defining the model
% Set the netword dimensions
[InputSize, SeqLength, N_Samples] = size(Data);

% Training options
options = trainingOptions('adam', ...
    'MaxEpochs', Learning_Params.numEpochs, ...
    'MiniBatchSize', Model_Hyperparams.miniBatchSize, ...
    'Shuffle', 'every-epoch', ...
    'InitialLearnRate', Learning_Params.learnRate, ...
    'L2Regularization', Learning_Params.L2Regularization, ...
    'GradientDecayFactor', Learning_Params.gradientDecayFactor, ...
    'SquaredGradientDecayFactor', Learning_Params.squaredGradientDecayFactor, ...
    'GradientThreshold', Learning_Params.gradientThreshold, ...
    'Verbose', 0, ...
    'Plots', 'none');

%% Data parsing for train, validation and test
N_TrainValid = NumFolds*floor(.8*N_Samples/NumFolds);
shuffledIdx = randperm(N_Samples);

YTest = categorical(Label(shuffledIdx(N_TrainValid + 1:end)));

CVO = cvpartition(N_TrainValid, 'KFold', NumFolds);

%% Sorting Features for Selection
Scores = zeros(NumFolds, InputSize);
for i_KFold = 1:NumFolds
    XTemp = reshape(Data(:, :, shuffledIdx(CVO.test(i_KFold))), InputSize, SeqLength*N_TrainValid/NumFolds)';
    YTemp = repmat(Label(shuffledIdx(CVO.test(i_KFold))), 1, SeqLength)';
    YTemp = YTemp(:);
    % Apply feature selection algorithm
    switch SelectionMethod
        case 'MRMR'
            [~,  Scores(i_KFold, :)] = fscmrmr(XTemp, YTemp);
        case 'NCA'
            mdl = fscnca(XTemp, YTemp, 'Standardize',true);
            Scores(i_KFold, :) = mdl.FeatureWeights;
    end
end
clear YTemp
[~, SortedFeatures] = sort(mean(Scores), 2, 'descend');
Results.SortedFeatures = SortedFeatures;

Results.Accuracy.Valid = zeros(InputSize, NumFolds);
Results.Accuracy.Test = zeros(InputSize, NumFolds);
for i_KFold = 1:NumFolds
    YTrain = categorical(Label(shuffledIdx(CVO.training(i_KFold))));
    YValid = categorical(Label(shuffledIdx(CVO.test(i_KFold))));
    
    for i_Feature = 1:InputSize
        %% Training
        XTemp = squeeze(num2cell(Data(SortedFeatures(1:i_Feature), :, shuffledIdx(CVO.training(i_KFold))), [1 2]));
        
        % Model architecture
        layers = sequenceInputLayer(i_Feature);
        for i_LSTM = 1:Model_Hyperparams.numLSTMs -1
            layers = cat(1, layers, ...
                lstmLayer(Model_Hyperparams.numHiddenUnits(i_LSTM), 'OutputMode', 'sequence'), ...
                dropoutLayer(Learning_Params.DroupoutProb));
        end
        layers = cat(1, layers, [lstmLayer(Model_Hyperparams.numHiddenUnits(end), 'OutputMode', 'last')
            fullyConnectedLayer(Model_Hyperparams.numClasses)
            softmaxLayer
            classificationLayer]);
        
        % Train
        net = trainNetwork(XTemp, YTrain, layers, options);
        
        %% Evaluation
        % Accuracy over validation samples
        XTemp = squeeze(num2cell(Data(SortedFeatures(1:i_Feature), :, shuffledIdx(CVO.test(i_KFold))), [1 2]));
        YPred = classify(net, XTemp, 'MiniBatchSize', Model_Hyperparams.miniBatchSize);
        Results.Accuracy.Valid(i_Feature, i_KFold) = 100*sum(YPred == YValid)./numel(YValid);
        % Accuracy over test samples
        XTemp = squeeze(num2cell(Data(SortedFeatures(1:i_Feature), :, shuffledIdx(N_TrainValid + 1:end)), [1 2]));
        YPred = classify(net, XTemp, 'MiniBatchSize', Model_Hyperparams.miniBatchSize);
        Results.Accuracy.Test(i_Feature, i_KFold) = 100*sum(YPred == YTest)./numel(YTest);
        
        % Displaying the result
        fprintf('Accuracy = [%4.2f  %4.2f]\t Number of Features = %d\t Fold = %d\n', ...
            Results.Accuracy.Valid(i_Feature, i_KFold), ...
            Results.Accuracy.Test(i_Feature, i_KFold), i_Feature, i_KFold)
        % Saving the result
        save([ResultsFile '.mat'], 'Results', 'Settings', 'Comment');
    end
end
% Ploting average results
figure('color', 'w'); grid on; hold on
plot(1:InputSize, mean(Results.Accuracy.Valid, 2), 'LineWidth', 4);
plot(1:InputSize, mean(Results.Accuracy.Test, 2), 'LineWidth', 4);
xlim([1 InputSize])
ylabel('Accuracy', 'FontSize', 18)
xlabel('Number of Selected Features', 'FontSize', 18)
legend('Validation Data', 'Test Data', 'FontSize', 16)
