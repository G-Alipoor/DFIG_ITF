% Testing classical methods for Inter-Turn Short-Circuit Fault Detection
%     in the DFIG's Stator using the proposed EMD-Based Statisctical Features, using
%     K-fold cross-validation
% This routine can be used to reproduce the results reported in the last 6 columns of tables 4 and 6
%     of the following paper:
%     Ghasem Alipoor, Seyed Jafar Mirbagheri, Seyed Mohammad Mahdi Moosavi and Sergio M. A. Cruz,
%     “Incipient Detection of Stator Inter-Turn Short-Circuit Faults in a DFIG Using Deep Learning,”
%     accepted for publication in the IET Electric Power Applications, DOI: 10.1049/elp2.12262.
% 
% Generated by G. Alipoor (alipoor@hut.ac.ir)
% Last Modifications October, 23rd, 2022
%

clearvars
close all
clc

%% Settings
TestVersion = '';

rng('default');

InputDirectory = 'Dataset';

Frame_Length = 360;
NumIMFs = 5;
NumFolds = 10;
NofSelectedFeatures = 150;
% An arbitrary comment, to saved with results
Comment = 'Learning parameters of varios classification methods can be found in the code.';

ResultsFile = sprintf('Results\\Classical_FrmLen%d_%dFeatures_%s', ...
    Frame_Length, NofSelectedFeatures, TestVersion);
% Settings, to be saved with results
Settings = struct('SelectedSignalsIndices', SelectedSignalsIndices, ...
    'NofSelectedFeatures', NofSelectedFeatures, ...
    'Frame_Length', Frame_Length, 'NumIMFs', NumIMFs, 'NumFolds', NumFolds);

%% Load Data
FeaturesFile = sprintf('Features\\Features_FrmLen%d_WholeFrames.mat', Frame_Length);
if ~exist(FeaturesFile, 'file')
    FeatureExtraction(InputDirectory, Frame_Length, NumIMFs)
end
Features = load(FeaturesFile, 'Data', 'Label', 'Comment');
Data  = Features.Data;
Label = Features.Label;
% Data = Data(:, 1:20:end); Label = Label(1:20:end);
Comment = sprintf('%s\n %s', Comment, Features.Comment);
clear Features

if NofSelectedFeatures ~= 150
    % Load the features sorted based on filter and hybrid feature selection methods
    Temp = load(sprintf('Results\\FeatureSelection_Hybrid_FrmLen%d_%s', ...
        Frame_Length), 'Results');
    SelectedFeatures = Temp.Results.SelectedFeatures(1:NofSelectedFeatures);
    clear Temp
    Data = Data(SelectedFeatures, :, :);
end

%% Classification
CVO = cvpartition(size(Data, 2),'KFold',NumFolds);  % Cross-Validation Object
for i_KFold = 1:NumFolds
    XTrain = Data(:, CVO.training(i_KFold));
    YTrain = Label(CVO.training(i_KFold));
    
    XTest = Data(:, CVO.test(i_KFold));
    YTest = Label(CVO.test(i_KFold));
    
    fprintf('Fold %d:\n', i_KFold)           % Displaying the current result
    
    % Linear Discriminant Analysis (LDA) classifier
    LearnerTemplate = templateDiscriminant('DiscrimType', 'linear');    % Specifying the binary classifier used
    classifier = fitcecoc(XTrain, YTrain, 'ObservationsIn', 'columns', 'Learners', LearnerTemplate);   % Train
    YPred = predict(classifier, XTest');                                                                                           % Test
    Accuracy.LDA(i_KFold) = 100*mean(YPred == YTest);                    % Accuracy for the current fold
    fprintf('\t LDA Accuracy   = %4.2f%%\n', Accuracy.LDA(i_KFold))    % Displaying the current result
    save(ResultsFile, 'Accuracy', 'Settings', 'Comment');              % Saving the current result
    
    % K-Nearest Neighbors (KNN) classifier
    K = 2*floor(round(sqrt(size(Data, 2)*(NumFolds - 1)/NumFolds))/2) + 1;
    LearnerTemplate = templateKNN('NumNeighbors', K);    % Specifying the binary classifier used
    classifier = fitcecoc(XTrain, YTrain, 'ObservationsIn', 'columns', 'Learners', LearnerTemplate);   % Train
    YPred = predict(classifier, XTest');                                                                                           % Test
    Accuracy.KNN(i_KFold) = 100*mean(YPred == YTest);                    % Accuracy for the current fold
    fprintf('\t KNN Accuracy   = %4.2f%%\n', Accuracy.KNN(i_KFold))   % Displaying the current result
    save(ResultsFile, 'Accuracy', 'Settings', 'Comment');                % Saving the current result
    
    % Naive Bayes classifier
    LearnerTemplate = templateNaiveBayes();    % Specifying the binary classifier used
    classifier = fitcecoc(XTrain, YTrain, 'ObservationsIn', 'columns', 'Learners', LearnerTemplate);   % Train
    YPred = predict(classifier, XTest');                                                                                           % Test
    Accuracy.Bayes(i_KFold) = 100*mean(YPred == YTest);                    % Accuracy for the current fold
    fprintf('\t Bayes Accuracy = %4.2f%%\n', Accuracy.Bayes(i_KFold))   % Displaying the current result
    save(ResultsFile, 'Accuracy', 'Settings', 'Comment');                 % Saving the current result
    
    % Support Vector Machine (SVM) classifier
    LearnerTemplate = templateSVM('KernelFunction', 'rbf');    % Specifying the binary classifier used
    classifier = fitcecoc(XTrain', YTrain, 'Learners', LearnerTemplate);   % Train
    YPred = predict(classifier, XTest');                                                  % Test
    Accuracy.SVM(i_KFold) = 100*mean(YPred == YTest);                    % Accuracy for the current fold
    fprintf('\t SVM Accuracy   = %4.2f%%\n', Accuracy.SVM(i_KFold))   % Displaying the current result
    save(ResultsFile, 'Accuracy', 'Settings', 'Comment');                % Saving the current result
    
    % Decision Tree classifier
    LearnerTemplate = templateTree();    % Specifying the binary classifier used
    classifier = fitcecoc(XTrain, YTrain, 'ObservationsIn', 'columns', 'Learners', LearnerTemplate);   % Train
    YPred = predict(classifier, XTest');                                                                                           % Test
    Accuracy.Tree(i_KFold) = 100*mean(YPred == YTest);                    % Accuracy for the current fold
    fprintf('\t Tree Accuracy  = %4.2f%%\n', Accuracy.Tree(i_KFold))    % Displaying the current result
    save(ResultsFile, 'Accuracy', 'Settings', 'Comment');               % Saving the current result
    
    % Multi-Layer Perceptron (MLP) classifier
    inputSize = size(XTrain, 1);    % Input dimensions
    net = feedforwardnet(10);     % Network architecture
    YTrain2 = oneHot(YTrain, length(unique(YTrain)));   % One-hot encoding of the target signal
    net = train(net, XTrain, YTrain2);                              % Train network
    [~, Temp] = max(net(XTest));                                   % Test network
    YPred = Temp' - 1;
    Accuracy.MLP(i_KFold) = 100*mean(YPred == YTest);                    % Accuracy for the current fold
    fprintf('\t MLP Accuracy   = %4.2f%%\n', Accuracy.MLP(i_KFold))   % Displaying the current result
    save(ResultsFile, 'Accuracy', 'Settings', 'Comment');               % Saving the current result
end

fprintf('=====================================\n')
fprintf('Averaged Results:\n')
fprintf('\tLDA: \tmean=%4.2f%%\t\tstd=%4.2f%%\n',mean(Accuracy.LDA), std(Accuracy.LDA))
fprintf('\tKNN: \tmean=%4.2f%%\t\tstd=%4.2f%%\n',mean(Accuracy.KNN), std(Accuracy.KNN))
fprintf('\tBayes:\tmean=%4.2f%%\t\tstd=%4.2f%%\n',mean(Accuracy.Bayes), std(Accuracy.Bayes))
fprintf('\tSVM: \tmean=%4.2f%%\t\tstd=%4.2f%%\n',mean(Accuracy.SVM), std(Accuracy.SVM))
fprintf('\tTree:\tmean=%4.2f%%\t\tstd=%4.2f%%\n',mean(Accuracy.Tree), std(Accuracy.Tree))
fprintf('\tMLP:\tmean=%4.2f%%\t\tstd=%4.2f%%\n',mean(Accuracy.MLP), std(Accuracy.MLP))

function oh = oneHot(idx, numTokens)
% One-Hot Encoding Function
% The oneHot function encodes word indices as one-hot vectors.

tokens = (0:numTokens-1)';
if size(idx,2) == 1
    idx = idx';
end
oh = (tokens == idx);
end
